{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_8_latest__MIS_536.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2deD3X91QvXI",
        "XS_XEYZuuacf",
        "c1CQGq8vy73e",
        "nFH9cVzGzfcf",
        "-l6ZY6I_zm7t",
        "YXH7YezV1hoQ",
        "qbfJhzAS12fp",
        "ZfXQtnyd6a2J"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepa2909/MLP-Classifier-K-NN---Compare-Performance-/blob/main/MLP%20and%20K-NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2deD3X91QvXI"
      },
      "source": [
        "# Task Description\n",
        "\n",
        "Fit an MLP classifier to the Universal Bank Data\n",
        "\n",
        "Clean and prepare the data (you’ve done this a few times now)\n",
        "\n",
        "Fit an MLP classifier to the data\n",
        "\n",
        "Experiment with different combinations neurons and hidden layers. \n",
        "\n",
        "Show at least 3\n",
        "\n",
        "Analyze performance of each of the three models on validation data\n",
        "\n",
        "Fit a k-nn model\n",
        "\n",
        "•use GridSearchCV to find optimal k\n",
        "\n",
        "•analyze performance of model on validation data\n",
        "\n",
        " Discuss findings \n",
        "\n",
        "•How does the predictive performance of k-nn model compare to the MLP classifier?\n",
        "\n",
        "•Which model takes less time to train? Which model is faster at prediction?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT51eQJquXjl"
      },
      "source": [
        "In this assignment, the task is to fit an MLP classifier on the Universal bank data. The predictor in the dataset is correctly predict 'personal loan' customer for the bank."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS_XEYZuuacf"
      },
      "source": [
        "# Import required packages "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5UADkx6DLFw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b551297-18b7-427d-b832-cb5e08caa1b9"
      },
      "source": [
        "!pip install dmba"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dmba\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/06/15c89846de47be5f3522ef0bebd61c62e14afe167877778094a335df5fe4/dmba-0.0.18-py3-none-any.whl (11.8MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8MB 243kB/s \n",
            "\u001b[?25hInstalling collected packages: dmba\n",
            "Successfully installed dmba-0.0.18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G70AUJFuYjI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acb3f1c4-c5e1-41e0-9839-c0dbd1e9f9f2"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing \n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "import time\n",
        "from dmba import classificationSummary\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no display found. Using non-interactive Agg backend\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1CQGq8vy73e"
      },
      "source": [
        "# Load the data direct from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhB8rQnmy8oy"
      },
      "source": [
        "\n",
        "bank_df = pd.read_csv('https://github.com/timcsmith/MIS536-Public/raw/master/Data/UniversalBank.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GACffMMzIzP"
      },
      "source": [
        "Quickly explore the data and fix any obvious problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRUrvt3DzQOA"
      },
      "source": [
        "\n",
        "Here, I explore the number of columns, see what the columns names look like (and remove whitespace and rename when it will make it easier to work with, and check occurances of NaN (missing values)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyq1AhJuzL3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2348324b-9504-4fa4-f28b-872b71bb3cde"
      },
      "source": [
        "bank_df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ID', 'Age', 'Experience', 'Income', 'ZIP Code', 'Family', 'CCAvg',\n",
              "       'Education', 'Mortgage', 'Personal Loan', 'Securities Account',\n",
              "       'CD Account', 'Online', 'CreditCard'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFH9cVzGzfcf"
      },
      "source": [
        "# let's replace any spaces in the column names with underscore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDAGHl8ezdkc"
      },
      "source": [
        "\n",
        "bank_df.columns = [s.strip().replace(' ','_') for s in bank_df.columns] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l6ZY6I_zm7t"
      },
      "source": [
        "# Let's check to see if there is a problem with missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8q0MLpizvtf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a854e2b3-a548-47e6-faaa-f49d44421fce"
      },
      "source": [
        "\n",
        "bank_df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ID                    0\n",
              "Age                   0\n",
              "Experience            0\n",
              "Income                0\n",
              "ZIP_Code              0\n",
              "Family                0\n",
              "CCAvg                 0\n",
              "Education             0\n",
              "Mortgage              0\n",
              "Personal_Loan         0\n",
              "Securities_Account    0\n",
              "CD_Account            0\n",
              "Online                0\n",
              "CreditCard            0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2VeoobPz130"
      },
      "source": [
        "\n",
        "There are no issues with missing values in this data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjiG6diY0BBX"
      },
      "source": [
        "We can also look at the proportion of the data for each of our target variables -- we find that this is somewhere around 9.6%, but since we have a rather large sample (5000), we will have many records with both clssification, so this should be fine.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhuRq91Q1Y4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c56f5561-c681-4501-ff34-5e9fecc64bb1"
      },
      "source": [
        "bank_df['Personal_Loan'].value_counts(normalize=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.904\n",
              "1    0.096\n",
              "Name: Personal_Loan, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXH7YezV1hoQ"
      },
      "source": [
        "# Drop the ID and zip code columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykBKAq701ywZ"
      },
      "source": [
        "bank_df = bank_df.drop(columns=['ID', 'ZIP_Code'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbfJhzAS12fp"
      },
      "source": [
        "# Create dummy variables for Education"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-INJBJaE16u_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "2c11f109-9099-472f-da26-fdcdf6f9b97c"
      },
      "source": [
        "bank_df['Education'] = bank_df['Education'].astype('category')\n",
        "bank_df = pd.get_dummies(bank_df, prefix_sep='_', drop_first=False)\n",
        "bank_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Experience</th>\n",
              "      <th>Income</th>\n",
              "      <th>Family</th>\n",
              "      <th>CCAvg</th>\n",
              "      <th>Mortgage</th>\n",
              "      <th>Personal_Loan</th>\n",
              "      <th>Securities_Account</th>\n",
              "      <th>CD_Account</th>\n",
              "      <th>Online</th>\n",
              "      <th>CreditCard</th>\n",
              "      <th>Education_1</th>\n",
              "      <th>Education_2</th>\n",
              "      <th>Education_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>25</td>\n",
              "      <td>1</td>\n",
              "      <td>49</td>\n",
              "      <td>4</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>45</td>\n",
              "      <td>19</td>\n",
              "      <td>34</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>39</td>\n",
              "      <td>15</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>35</td>\n",
              "      <td>9</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>2.7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>35</td>\n",
              "      <td>8</td>\n",
              "      <td>45</td>\n",
              "      <td>4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Age  Experience  Income  ...  Education_1  Education_2  Education_3\n",
              "0   25           1      49  ...            1            0            0\n",
              "1   45          19      34  ...            1            0            0\n",
              "2   39          15      11  ...            1            0            0\n",
              "3   35           9     100  ...            0            1            0\n",
              "4   35           8      45  ...            0            1            0\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfXQtnyd6a2J"
      },
      "source": [
        "# Split dataset into training (70%) and validation (30%) sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv-m80lY59nj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d3feb35-b635-403f-93d1-1b8c3efd72c9"
      },
      "source": [
        "\n",
        "target = 'Personal_Loan'\n",
        "predictors = list(bank_df.columns)\n",
        "predictors.remove(target)\n",
        "X = bank_df[predictors]\n",
        "y = bank_df[target]\n",
        "train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=.3, random_state=1)\n",
        "print('Training set:', train_X.shape, 'Validation set:', valid_y.shape)\n",
        "print(\"train_X\", train_X.shape)\n",
        "print(\"train_Y\", train_y.shape)\n",
        "print(\"valid_X\", valid_X.shape)\n",
        "print(\"valid_y\", valid_y.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set: (3500, 13) Validation set: (1500,)\n",
            "train_X (3500, 13)\n",
            "train_Y (3500,)\n",
            "valid_X (1500, 13)\n",
            "valid_y (1500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ML_IMzi99NK_"
      },
      "source": [
        "# Fit MLPClassifier (Multi-Layer Perceptron Classifier)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3XY3awCvZ4F"
      },
      "source": [
        "param_grid_rand = {\n",
        "'hidden_layer_sizes': [(450,350,250), (700,350,200,80), (250,275,400)], # this is a list of different tuples\n",
        "'activation': ['tanh', 'relu'],\n",
        "'solver': ['sgd', 'adam'],\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxKEXqkPb7aH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "794d215b-400f-48ae-deb2-068aa0017134"
      },
      "source": [
        "#combination 1\n",
        "%%time\n",
        "start = time.time()\n",
        "\n",
        "model = MLPClassifier()\n",
        "randomSearch = RandomizedSearchCV(estimator = model, param_distributions  = param_grid_rand,  cv = 3, verbose=2, n_jobs = -1)\n",
        "randomSearch.fit(train_X, train_y)\n",
        "\n",
        "bestgridmodel_mlp_1 = randomSearch.best_estimator_\n",
        "print('Best parameters found: ', randomSearch.best_params_)\n",
        "\n",
        "\n",
        "end = time.time()\n",
        "print(\"Total Time\", end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 12.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best parameters found:  {'solver': 'adam', 'hidden_layer_sizes': (450, 350, 250), 'activation': 'tanh'}\n",
            "Total Time 802.3188631534576\n",
            "CPU times: user 1min 26s, sys: 37.7 s, total: 2min 4s\n",
            "Wall time: 13min 22s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULlbsn5lcBNZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "c22b4f37-f26f-45de-f30c-f7834b9dff06"
      },
      "source": [
        "%%time\n",
        "start = time.time()\n",
        "\n",
        "sample_X = valid_X.iloc[1]\n",
        "sample_X = sample_X.values.reshape(1, -1)\n",
        "print(sample_X)\n",
        "print(model.predict(sample_X))\n",
        "\n",
        "end = time.time()\n",
        "print(\"Total Time\", end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 35.    9.   45.    3.    0.9 101.    1.    0.    0.    0.    1.    0.\n",
            "    0. ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NotFittedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a6665e448e9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'start = time.time()\\n\\nsample_X = valid_X.iloc[1]\\nsample_X = sample_X.values.reshape(1, -1)\\nprint(sample_X)\\nprint(model.predict(sample_X))\\n\\nend = time.time()\\nprint(\"Total Time\", end - start)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m         \"\"\"\n\u001b[0;32m--> 970\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFittedError\u001b[0m: This MLPClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPJEtBTCcFze"
      },
      "source": [
        "%%time\n",
        "\n",
        "start = time.time()\n",
        "validation_predictions = model.predict(valid_X)\n",
        "end = time.time()\n",
        "print(\"Total Time per prediction =\", (end - start)/len(valid_X))\n",
        "\n",
        "print('confusion_matrix:\\n ', confusion_matrix(valid_y, validation_predictions))\n",
        "print('Accuracy Score: ', accuracy_score(valid_y, validation_predictions))\n",
        "#print('Precision Score: ', precision_score(valid_y, validation_predictions))\n",
        "print('Recall Score: ', recall_score(valid_y, validation_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYSp9b4yTUlw"
      },
      "source": [
        "param_grid = {\n",
        "'hidden_layer_sizes': [(250,350,50), (500,250,100,40), (150,175,100)], # this is a list of different tuples\n",
        "'activation': ['tanh', 'relu'],\n",
        "'solver': ['sgd', 'adam'],\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyQ42g9y9TgL"
      },
      "source": [
        "#combination 2\n",
        "%%time\n",
        "start = time.time()\n",
        "\n",
        "model = MLPClassifier()\n",
        "gridSearch = GridSearchCV(estimator = model, param_grid = param_grid,  cv = 3, verbose=2, n_jobs = -1)\n",
        "gridSearch.fit(train_X, train_y)\n",
        "\n",
        "bestgridmodel_mlp_1 = gridSearch.best_estimator_\n",
        "print('Best parameters found: ', gridSearch.best_params_)\n",
        "\n",
        "end = time.time()\n",
        "print(\"Total Time\", end - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-61UGlA9zFg"
      },
      "source": [
        "%%time\n",
        "start = time.time()\n",
        "\n",
        "sample_X = valid_X.iloc[1]\n",
        "sample_X = sample_X.values.reshape(1, -1)\n",
        "print(sample_X)\n",
        "print(gridSearch.predict(sample_X))\n",
        "\n",
        "end = time.time()\n",
        "print(\"Total Time\", end - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNzlS2X1AZlH"
      },
      "source": [
        "%%time\n",
        "\n",
        "start = time.time()\n",
        "validation_predictions = gridSearch.predict(valid_X)\n",
        "end = time.time()\n",
        "print(\"Total Time per prediction =\", (end - start)/len(valid_X))\n",
        "\n",
        "print('confusion_matrix:\\n ', confusion_matrix(valid_y, validation_predictions))\n",
        "print('Accuracy Score: ', accuracy_score(valid_y, validation_predictions))\n",
        "#print('Precision Score: ', precision_score(valid_y, validation_predictions))\n",
        "print('Recall Score: ', recall_score(valid_y, validation_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xmdjlt_ZNRY"
      },
      "source": [
        "\n",
        "param_grid2 = {\n",
        "'hidden_layer_sizes': [(110,50,20), (260,200,140, 120), (360,325,300)], \n",
        "'activation': ['tanh'],\n",
        "'solver': ['adam'],\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9sffC5TEY97"
      },
      "source": [
        "\n",
        "#combination 3\n",
        "\n",
        "%%time\n",
        "start = time.time()\n",
        "\n",
        "model = MLPClassifier()\n",
        "gridSearch = GridSearchCV(estimator = model, param_grid = param_grid2,  cv = 3, verbose=2, n_jobs = -1)\n",
        "gridSearch.fit(train_X, train_y)\n",
        "\n",
        "bestgridmodel_mlp_1 = gridSearch.best_estimator_\n",
        "print('Best parameters found: ', gridSearch.best_params_)\n",
        "\n",
        "end = time.time()\n",
        "print(\"Total Time\", end - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZ6IRXacEeUQ"
      },
      "source": [
        "%%time\n",
        "start = time.time()\n",
        "\n",
        "sample_X = valid_X.iloc[1]\n",
        "sample_X = sample_X.values.reshape(1, -1)\n",
        "print(sample_X)\n",
        "print(gridSearch.predict(sample_X))\n",
        "\n",
        "end = time.time()\n",
        "print(\"Total Time\", end - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcK753XcEiMj"
      },
      "source": [
        "%%time\n",
        "\n",
        "start = time.time()\n",
        "validation_predictions_2 = gridSearch.predict(valid_X)\n",
        "end = time.time()\n",
        "print(\"Total Time per prediction =\", (end - start)/len(valid_X))\n",
        "\n",
        "print('confusion_matrix:\\n ', confusion_matrix(valid_y, validation_predictions_2))\n",
        "print('Accuracy Score: ', accuracy_score(valid_y, validation_predictions_2))\n",
        "#print('Precision Score: ', precision_score(valid_y, validation_predictions_2))\n",
        "print('Recall Score: ', recall_score(valid_y, validation_predictions_2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ3x_bp80OFZ"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slNQa0-46BPS"
      },
      "source": [
        "\n",
        " # create a standard scaler and fit it to the training set of predictors For KNN model\n",
        "scaler = preprocessing.StandardScaler()\n",
        "scaler.fit(train_X)\n",
        "\n",
        "# Transform the predictors of training and validation sets\n",
        "train_X_knn = scaler.transform(train_X)\n",
        "train_y_knn = train_y\n",
        "valid_X_knn = scaler.transform(valid_X)\n",
        "valid_y_knn = valid_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGfcwptZTMHy"
      },
      "source": [
        "\n",
        "# let's explore the performance of 5-NN model\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(train_X_knn, train_y_knn)\n",
        "knn_prediction_output = knn.predict(valid_X_knn)\n",
        "confusion = confusion_matrix(valid_y_knn, knn_prediction_output)\n",
        "confusion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkF3OmOfH_oJ"
      },
      "source": [
        "%%time\n",
        "start = time.time()\n",
        "#List Hyperparameters that we want to tune.\n",
        "leaf_size = list(range(1,500))\n",
        "n_neighbors = list(range(1,300))\n",
        "p=[1,2]\n",
        "#Convert to dictionary\n",
        "hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
        "#Create new KNN object\n",
        "knn_1 = KNeighborsClassifier()\n",
        "#Use GridSearch\n",
        "clf = RandomizedSearchCV(knn_1, hyperparameters, cv=10)\n",
        "#Fit the model\n",
        "best_model = clf.fit(train_X_knn,train_y_knn)\n",
        "#Print The value of best Hyperparameters\n",
        "\n",
        "bestRandomModel = best_model.best_estimator_\n",
        "print('Best parameters found: ', clf.best_params_)\n",
        "\n",
        "end = time.time()\n",
        "print(\"Total Time\", end - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncLOXqbaDeO2"
      },
      "source": [
        "%%time\n",
        "\n",
        "start = time.time()\n",
        "validation_predictions_knn_1 = bestRandomModel.predict(valid_X_knn)\n",
        "end = time.time()\n",
        "print(\"Total Time per prediction =\", (end - start)/len(valid_X_knn))\n",
        "\n",
        "\n",
        "print('Confusion Matrix: \\n', confusion_matrix(valid_y_knn, validation_predictions_knn_1))\n",
        "print('Accuracy: ', accuracy_score(valid_y_knn, validation_predictions_knn_1))\n",
        "print('Precision: ', precision_score(valid_y_knn, validation_predictions_knn_1))\n",
        "print('Recall: ', recall_score(valid_y_knn, validation_predictions_knn_1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izxWayAE8aIX"
      },
      "source": [
        "%%time\n",
        "start = time.time()\n",
        "\n",
        "#List Hyperparameters that we want to tune.\n",
        "leaf_size = list(range(350,390))\n",
        "n_neighbors = list(range(80,120))\n",
        "p=[1,2]\n",
        "#Convert to dictionary\n",
        "hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
        "#Create new KNN object\n",
        "knn_3 = KNeighborsClassifier()\n",
        "#Use GridSearch\n",
        "clf_3 = GridSearchCV(knn_3, hyperparameters, cv=10)\n",
        "#Fit the model\n",
        "best_model_3 = clf_3.fit(train_X_knn,train_y_knn)\n",
        "#Print The value of best Hyperparameters\n",
        "bestRandomModel_3 = best_model_3.best_estimator_\n",
        "print('Best parameters found: ', clf_3.best_params_)\n",
        "end = time.time()\n",
        "print(\"Total Time\", end - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YIJIa95EQNz"
      },
      "source": [
        "%%time\n",
        "\n",
        "start = time.time()\n",
        "validation_predictions_knn_3 = bestRandomModel_3.predict(valid_X_knn)\n",
        "end = time.time()\n",
        "print(\"Total Time per prediction =\", (end - start)/len(valid_X_knn))\n",
        "\n",
        "\n",
        "print('Confusion Matrix: \\n', confusion_matrix(valid_y_knn, validation_predictions_knn_3))\n",
        "print('Accuracy: ', accuracy_score(valid_y_knn, validation_predictions_knn_3))\n",
        "print('Precision: ', precision_score(valid_y_knn, validation_predictions_knn_3))\n",
        "print('Recall: ', recall_score(valid_y_knn, validation_predictions_knn_3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xB4PBqv5_gUg"
      },
      "source": [
        "%%time\n",
        "start = time.time()\n",
        "\n",
        "#List Hyperparameters that we want to tune.\n",
        "leaf_size = list(range(350,380))\n",
        "n_neighbors = list(range(60,80))\n",
        "p=[1,2]\n",
        "#Convert to dictionary\n",
        "hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
        "#Create new KNN object\n",
        "knn_4 = KNeighborsClassifier()\n",
        "#Use GridSearch\n",
        "clf_4 = GridSearchCV(knn_4, hyperparameters, cv=10)\n",
        "#Fit the model\n",
        "best_model_4 = clf_4.fit(train_X_knn,train_y_knn)\n",
        "\n",
        "#Print The value of best Hyperparameters\n",
        "bestRandomModel_4 = best_model_4.best_estimator_\n",
        "print('Best parameters found: ', clf_3.best_params_)\n",
        "end = time.time()\n",
        "print(\"Total Time\", end - start)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pIrdtxkRl-j"
      },
      "source": [
        "%%time\n",
        "\n",
        "start = time.time()\n",
        "validation_predictions_knn_4 = bestRandomModel_4.predict(valid_X_knn)\n",
        "end = time.time()\n",
        "print(\"Total Time per prediction =\", (end - start)/len(valid_X_knn))\n",
        "\n",
        "\n",
        "print('Confusion Matrix: \\n', confusion_matrix(valid_y_knn, validation_predictions_knn_4))\n",
        "print('Accuracy: ', accuracy_score(valid_y_knn, validation_predictions_knn_4))\n",
        "print('Precision: ', precision_score(valid_y_knn, validation_predictions_knn_4))\n",
        "print('Recall: ', recall_score(valid_y_knn, validation_predictions_knn_4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciJWPjS2TDsk"
      },
      "source": [
        "## Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBLB5dqeTHhE"
      },
      "source": [
        "So I tried three combinations of parameters for each model i.e., neural network and KNN both. \n",
        "one with randomsearch and two with gridsearchcv hyperparameter tuning.\n",
        "\n",
        "1. Which model takes less time to train? Which model is faster at prediction?\n",
        "\n",
        "\n",
        "> Time taken by each model is listed below:\n",
        "      \n",
        "\n",
        "      > **Neural Network**: \n",
        "          Time taken to train:\n",
        "          > Random search -  total: 1min 5s\n",
        "          > Grid Search 1 -  total: 48.9 s\n",
        "          > Grid Search 2 - total: 10.2 s\n",
        "          Time taken for prediction:\n",
        "          > Random search -  total: 81.7 ms\n",
        "          > Grid Search 1 -  total: 34.8 ms \n",
        "          > Grid Search 2 -  total: 54.1 ms\n",
        "      > **KNN**:\n",
        "          Time taken to train:\n",
        "          > Random search -  total: 5.67 s\n",
        "          > Grid Search 1 -  total: 27min 9s\n",
        "          > Grid Search 2 -  total: 9min 22s\n",
        "          Time taken for prediction:\n",
        "          > Random search -  total: 206 ms\n",
        "          > Grid Search 1 -  total: 196 ms\n",
        "          > Grid Search 2 -  total: 179 ms\n",
        "\n",
        "\n",
        "Time taken by KNN for prediction is way more than the neural network. So, Neural network is faster at predictions. Also, the time taken for training KNN classifier is way higher than MLP classifier, so I conclude that MLP classifier is faster than KNN. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXyNaGLUZtV0"
      },
      "source": [
        "# 2. How does the predictive performance of k-nn model compare to the MLP classifier?\n",
        "\n",
        "# 2.1 MLP Classifier\n",
        "    Random Search \n",
        "    confusion_matrix:\n",
        "      [[1332   19]\n",
        "      [  20  129]]\n",
        "    Accuracy Score:  0.974\n",
        "    Recall Score:  0.8657718120805369\n",
        "On running random search the neural network, MLP classifier, does a fine job and has a recall of 86.577. Our aim is to increase this score, so, the model predicts less and less FN so we the business doesn't lose any customer by false prediction. \n",
        "We will chose a model that will have the highest recall score, which will indicate the model is predicting less and less falsely judging a potential customer a not a customer for personal loan. \n",
        "\n",
        "    Grid Search 1\n",
        "    confusion_matrix:\n",
        "      [[1339   12]\n",
        "      [  31  118]]\n",
        "    Accuracy Score:  0.9713333333333334\n",
        "    Recall Score:  0.7919463087248322\n",
        "We notice that by hyperparameter tunning parameter further, both the accuracy and recall score is reduced. So, this should not be considered because this model does not do better than the above random search model. \n",
        "\n",
        "    Grid Search 1\n",
        "    confusion_matrix:\n",
        "      [[1340   11]\n",
        "      [  36  113]]\n",
        "    Accuracy Score:  0.9686666666666667\n",
        "    Recall Score:  0.7583892617449665\n",
        "We notice again, the hyperparameter tuning is not helping improve the performance of the model further. Therefore, I decided to stop further tuning parameters further. I accepted the randomsearch parameter tuning and performance of the model and accepted to compare with the KNN model with random search and grid search hyperparameter tuning of model for further investigation. \n",
        "\n",
        "# 2.2 KNN \n",
        "I initially fitted and run a KNN model with default parameter and confusion matrix comes out to be:\n",
        "    Confusion Matrix:\n",
        "    array([[1347,    4],\n",
        "       [  62,   87]])\n",
        "\n",
        "It looks like it is doing a fine job at predicting FN, i.e, recall score, which is the requirement of the business model. But, it is still not doing better than random search MLP classifier. So, I decided to further investigate using randomsearch and gridsearch hyperparameter to conclude something about the performance of both the classifier and tuning method used.\n",
        "\n",
        "#    Random search \n",
        "\n",
        "    Confusion Matrix: \n",
        "      [[1351    0]\n",
        "      [ 132   17]]\n",
        "    Accuracy:  0.912\n",
        "    Precision:  1.0\n",
        "    Recall:  0.11409395973154363\n",
        "\n",
        "\n",
        "#    Grid Search 1\n",
        "\n",
        "    Confusion Matrix: \n",
        "    [[1350    1]\n",
        "    [ 117   32]]\n",
        "    Accuracy:  0.9213333333333333\n",
        "    Precision:  0.9696969696969697\n",
        "    Recall:  0.21476510067114093\n",
        "# Grid Search 2\n",
        "\n",
        "    Confusion Matrix: \n",
        "    [[1350    1]\n",
        "    [ 105   44]]\n",
        "    Accuracy:  0.9293333333333333\n",
        "    Precision:  0.9777777777777777\n",
        "    Recall:  0.2953020134228188\n",
        "\n",
        "After comparing the confusion matrix for all i.e, random search, grid search 1 and 2 for KNN classifer, I can conclude that the hyperparameter tuning is not helping in improving the performance of the KNN model.But, by default parameters, it performed better, so it is a possibility that further investigating the gridsearch using a wide range of parameters may further improve the performance of the model. But, this process is time consuming. It is trade off between the time and better recall score performance of the model. \n",
        "\n",
        "Therfore, with current investigation, the MLP classifer with random search parameters performed the best. The time taken for training and prediction for MLP classifier is lesser compared to the KNN. Therefore, I recommend MLP classifier with random search model for the business model where the model should predict potential customer with less and less chance of losing a potential customer, i.e, with high recall score. \n",
        "\n"
      ]
    }
  ]
}